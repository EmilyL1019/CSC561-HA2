{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a41d0bc-3a02-430a-9037-9787e5775260",
   "metadata": {},
   "source": [
    "Emily, AJ, &\n",
    "Assignment 2\n",
    "CSC 561 - Professor Alvarez\n",
    "4/18/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f05c436-f387-4ef1-89c6-b86e1b07996c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.local/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847e10ec-7e33-4736-b00e-c56f7d54551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a6f1f65-f044-4cbe-a6a9-815d0d94067c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: 1000268201_693b08cb0e.jpg, Captions: ['A child in a pink dress is climbing up a set of stairs in an entry way .\\n', 'A girl going into a wooden building .\\n', 'A little girl climbing into a wooden playhouse .\\n', 'A little girl climbing the stairs to her playhouse .\\n', 'A little girl in a pink dress going into a wooden cabin .\\n']\n",
      "Image ID: 1002674143_1b742ab4b8.jpg, Captions: ['A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .\\n', 'A little girl is sitting in front of a large painted rainbow .\\n', 'A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .\\n', 'There is a girl with pigtails sitting in front of a rainbow painting .\\n', 'Young girl with pigtails painting outside in the grass .\\n']\n",
      "Image ID: 1003163366_44323f5815.jpg, Captions: ['A man lays on a bench while his dog sits by him .\\n', 'A man lays on the bench to which a white dog is also tied .\\n', 'a man sleeping on a bench outside with a white and black dog sitting next to him .\\n', 'A shirtless man lies on a park bench with his dog .\\n', 'man laying on bench holding leash of dog sitting on ground\\n']\n",
      "Image ID: 1007320043_627395c3d8.jpg, Captions: ['A child playing on a rope net .\\n', 'A little girl climbing on red roping .\\n', 'A little girl in pink climbs a rope bridge at the park .\\n', 'A small child grips onto the red ropes at the playground .\\n', 'The small child climbs on a red ropes on a playground .\\n']\n",
      "Image ID: 1009434119_febe49276a.jpg, Captions: ['A black and white dog is running in a grassy garden surrounded by a white fence .\\n', 'A black and white dog is running through the grass .\\n', 'A Boston terrier is running in the grass .\\n', 'A Boston Terrier is running on lush green grass in front of a white fence .\\n', 'A dog runs on the green grass near a wooden fence .\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/emily_light_uri_edu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set NLTK data path and download necessary files\n",
    "nltk_data_path = '/home/emily_light_uri_edu/nltk_data'\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Load captions from file\n",
    "caption_file = \"/work/pi_csc561_uri_edu/datasets/flickr/training-captions.json\"\n",
    "caption_dict = defaultdict(list)\n",
    "\n",
    "# Read caption data\n",
    "with open(caption_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Store captions by image\n",
    "for img_id, captions in data.items():\n",
    "    for caption in captions:\n",
    "        caption_dict[img_id].append(caption)\n",
    "        \n",
    "for img_id, captions in list(data.items())[:5]:\n",
    "    print(f\"Image ID: {img_id}, Captions: {captions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5e2fcb-70f7-4487-aa8e-90baac5fce98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2722\n"
     ]
    }
   ],
   "source": [
    "# Tokenize captions\n",
    "tokens = []\n",
    "for img_id, captions in caption_dict.items():\n",
    "    for caption in captions:\n",
    "        tokens += word_tokenize(caption.lower(), language='english')\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Counter(tokens)\n",
    "\n",
    "# Filter words with frequency less than 5\n",
    "min_freq = 5\n",
    "filtered_vocab = {word for word, count in vocab.items() if count >= min_freq}\n",
    "\n",
    "# Create word2idx and idx2word dictionaries\n",
    "word2idx = {\n",
    "    '<pad>': 0,\n",
    "    '<start>': 1,\n",
    "    '<end>': 2,\n",
    "    '<unk>': 3,\n",
    "}\n",
    "\n",
    "for word in sorted(filtered_vocab):\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Save vocab\n",
    "import pickle\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(word2idx, f)\n",
    "\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d66243f5-c297-4fcc-8d46-62825bec5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Load image features\n",
    "train_features = torch.load(\"train_features.pt\")\n",
    "\n",
    "# Get all image IDs with captions\n",
    "all_ids = list(caption_dict.keys())\n",
    "\n",
    "# Shuffle and split\n",
    "random.seed(42)\n",
    "random.shuffle(all_ids)\n",
    "split_idx = int(0.9 * len(all_ids))\n",
    "train_ids = all_ids[:split_idx]\n",
    "val_ids = all_ids[split_idx:]\n",
    "\n",
    "# Find captioned images without features\n",
    "missing_keys = [k for k in caption_dict if k not in train_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef1de00-358f-486b-8636-842129d5f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, captions, word2idx, max_len=30):\n",
    "        self.features = features\n",
    "        self.captions = captions\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "        self.keys = list(captions.keys())  # List of image IDs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.keys[idx]  # Get image ID from the list\n",
    "        img_feat = torch.tensor(self.features[img_id], dtype=torch.float32)\n",
    "\n",
    "        # Choose the first caption or a random one (if multiple)\n",
    "        caption_text = self.captions[img_id][0].lower()\n",
    "        tokens = word_tokenize(caption_text)\n",
    "\n",
    "        # Add start/end and map to indices\n",
    "        caption_idx = [self.word2idx['<start>']] + \\\n",
    "                      [self.word2idx.get(token, self.word2idx['<unk>']) for token in tokens] + \\\n",
    "                      [self.word2idx['<end>']]\n",
    "\n",
    "        # Pad or truncate\n",
    "        if len(caption_idx) < self.max_len:\n",
    "            caption_idx += [self.word2idx['<pad>']] * (self.max_len - len(caption_idx))\n",
    "        else:\n",
    "            caption_idx = caption_idx[:self.max_len]\n",
    "\n",
    "        caption_tensor = torch.tensor(caption_idx, dtype=torch.long)\n",
    "\n",
    "        return img_feat, caption_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4868fec9-a03d-4d66-9b03-d67a4067cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file, word2idx, embed_dim=300):\n",
    "    embedding_matrix = np.zeros((len(word2idx), embed_dim))\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            if word in word2idx:\n",
    "                embedding_matrix[word2idx[word]] = np.asarray(values[1:], dtype=np.float32)\n",
    "    return torch.tensor(embedding_matrix)\n",
    "\n",
    "# Path to your GloVe embeddings\n",
    "glove_file = 'glove.6B.300d.txt'\n",
    "\n",
    "# Assuming word2idx is already created and contains the vocabulary\n",
    "pretrained_embedding_matrix = load_glove_embeddings(glove_file, word2idx, embed_dim=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae0ca724-50d6-425f-9952-139d94c1230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 6381\n",
      "Number of validation samples: 710\n"
     ]
    }
   ],
   "source": [
    "# Exclude test images from train and val sets\n",
    "valid_train_ids = [k for k in train_ids if k not in missing_keys]\n",
    "valid_val_ids = [k for k in val_ids if k not in missing_keys]\n",
    "\n",
    "# Create datasets using valid IDs\n",
    "train_caps = {k: caption_dict[k] for k in valid_train_ids}\n",
    "val_caps = {k: caption_dict[k] for k in valid_val_ids}\n",
    "train_feats = {k: train_features[k] for k in valid_train_ids}\n",
    "val_feats = {k: train_features[k] for k in valid_val_ids}\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = CaptionDataset(train_feats, train_caps, word2idx)\n",
    "val_dataset = CaptionDataset(val_feats, val_caps, word2idx)\n",
    "\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42c3b69d-539a-4b8d-873b-90fb03273817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pad_idx = word2idx['<pad>']\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, vocab_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, captions in dataloader:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        outputs = model(images, captions)  # shape: (batch, seq_len, vocab_size)\n",
    "\n",
    "        outputs = outputs[:, :-1, :].reshape(-1, vocab_size)  # Remove the last token, flatten to [B*(T-1), V]\n",
    "        targets = captions[:, 1:].reshape(-1)  # Remove the first token (shifted target), flatten to [B*(T-1)]\n",
    "        non_pad = (targets != pad_idx).sum().item()\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, vocab_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_feats, captions in dataloader:\n",
    "            image_feats = image_feats.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            outputs = model(image_feats, captions)\n",
    "            outputs = outputs[:, :-1, :].reshape(-1, vocab_size)\n",
    "            targets = captions[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bc85632-dea4-4250-9d8f-ac77f2928933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, feature_dim, embed_dim, hidden_dim, vocab_size, pretrained_embedding_matrix=None):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        \n",
    "        # Initialize embedding layer with pretrained embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        if pretrained_embedding_matrix is not None:\n",
    "            self.embed.weight.data.copy_(pretrained_embedding_matrix)\n",
    "            self.embed.weight.requires_grad = False  # Freeze embeddings (optional)\n",
    "        \n",
    "        self.feat2hidden = nn.Linear(feature_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, image_feats, captions):\n",
    "        # Encode image features to initial hidden state\n",
    "        h0 = self.feat2hidden(image_feats).unsqueeze(0)  # (1, batch, hidden_dim)\n",
    "        c0 = torch.zeros_like(h0)  # Initial cell state (same shape as h0)\n",
    "        \n",
    "        # Embed captions\n",
    "        embeddings = self.embed(captions)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # Decode\n",
    "        outputs, _ = self.lstm(embeddings, (h0, c0))  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # Project to vocab\n",
    "        outputs = self.fc(outputs)  # (batch, seq_len, vocab_size)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61cea1ab-377e-402e-8a25-1ca218a3add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption example: tensor([  1,  18,  19, 306,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0])\n",
      "Decoded: ['<start>', '``', 'a', 'boy', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Batch 1:\n",
      "Images batch shape: torch.Size([32, 2048])\n",
      "Captions batch shape: torch.Size([32, 30])\n",
      "First caption in this batch: tensor([   1,   18,   19,  269, 1201, 1559, 2423, 2628,    2,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_637675/1531586754.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_feat = torch.tensor(self.features[img_id], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "images, captions = next(iter(train_loader))\n",
    "print(\"Caption example:\", captions[0])\n",
    "print(\"Decoded:\", [idx2word[i.item()] for i in captions[0] if i.item() in idx2word])\n",
    "\n",
    "for batch_idx, (images, captions) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx+1}:\")\n",
    "    print(f\"Images batch shape: {images.shape}\")  # Check the shape of the image tensor\n",
    "    print(f\"Captions batch shape: {captions.shape}\")  # Check the shape of the caption tensor\n",
    "    print(\"First caption in this batch:\", captions[0])  # Print the first caption to inspect\n",
    "    break  # Only view the first batch to avoid excessive printing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d04c6-c3f4-4bcf-9541-47934eb46a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_637675/1531586754.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_feat = torch.tensor(self.features[img_id], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model configuration\n",
    "feature_dim = 2048\n",
    "embed_dim = 300\n",
    "hidden_dim = 512\n",
    "vocab_size = len(word2idx)\n",
    "pad_idx = word2idx[\"<pad>\"]\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ImageCaptioningModel(feature_dim, embed_dim, hidden_dim, vocab_size, pretrained_embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, vocab_size)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss = evaluate(model, val_loader, criterion, vocab_size)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"✅ Saved new best model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ff75d-3169-48ba-8b0a-dc09db60e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image_feat, word2idx, idx2word, max_len=20):\n",
    "    model.eval()\n",
    "    caption = [word2idx[\"<start>\"]]\n",
    "    input_seq = torch.tensor([caption], dtype=torch.long).to(device)\n",
    "    image_feat = image_feat.to(device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    hidden = model.feat2hidden(image_feat).unsqueeze(0)  # (1, batch, hidden_dim)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        emb = model.embed(input_seq[:, -1])  # Last generated word\n",
    "        emb = emb.unsqueeze(1)  # (batch, 1, embed_dim)\n",
    "        output, hidden = model.gru(emb, hidden)\n",
    "        logits = model.fc(output.squeeze(1))  # (batch, vocab_size)\n",
    "        next_token = logits.argmax(-1).item()\n",
    "\n",
    "        if next_token == word2idx[\"<end>\"]:\n",
    "            break\n",
    "\n",
    "        result.append(idx2word.get(next_token, \"<unk>\"))\n",
    "        input_seq = torch.cat([input_seq, torch.tensor([[next_token]]).to(device)], dim=1)\n",
    "\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621bb376-1a0e-4221-a841-ce6439832976",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm-predictions.txt\", \"w\") as f:\n",
    "    for image_id, feature in tqdm(test_features.items(), desc=\"Generating captions\"):\n",
    "        feature_tensor = torch.tensor(feature, dtype=torch.float32).to(device)\n",
    "        caption = generate_caption(model, feature_tensor, word2idx, idx2word)\n",
    "        f.write(f\"{image_id}: {caption}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274926d-25e7-489f-ae57-c5a7fa96b35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
